{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the files and loading them into dataframes.\n",
    "train = pd.read_csv('C:/Users/Amine/Desktop/Standard Bank Tech Impact Challenge Xente credit scoring challenge/Train.csv')\n",
    "test= pd.read_csv('C:/Users/Amine/Desktop/Standard Bank Tech Impact Challenge Xente credit scoring challenge/Test.csv')\n",
    "sample = pd.read_csv('C:/Users/Amine/Desktop/Standard Bank Tech Impact Challenge Xente credit scoring challenge/sample_submission.csv')\n",
    "mask = pd.read_csv('C:/Users/Amine/Desktop/Standard Bank Tech Impact Challenge Xente credit scoring challenge/unlinked_masked_final.csv')\n",
    "variabs = pd.read_csv('C:/Users/Amine/Desktop/Standard Bank Tech Impact Challenge Xente credit scoring challenge/VariableDefinitions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform dates types from 'object' to 'datetime'\n",
    "train.TransactionStartTime=pd.to_datetime(train.TransactionStartTime)\n",
    "test.TransactionStartTime=pd.to_datetime(test.TransactionStartTime)\n",
    "train.IssuedDateLoan=pd.to_datetime(train.IssuedDateLoan)\n",
    "test.IssuedDateLoan=pd.to_datetime(test.IssuedDateLoan)\n",
    "train.PaidOnDate=pd.to_datetime(train.PaidOnDate)\n",
    "train.DueDate=pd.to_datetime(train.DueDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating variables to transfer the information contained in the rows of the same transaction.\n",
    "train['Number_Of_Split_Payments'] = 0 ## this is a count on the number of payments on the same loan. It will take a 0 for singled-rowed transactions, 1+ for multi-row transacs.\n",
    "#train['Sum_Diff_Time_Payments'] = 0 ## I'm thinking of summing the delays between all payments made on a loan. It will take 0 for loans paid in a single time, 1+ for multiple payments on the same loan.\n",
    "test['Number_Of_Split_Payments']=0\n",
    "#test['Sum_Diff_Time_Payments']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the feature : number of split payments on a loan.\n",
    "train['Number_Of_Split_Payments']=train['TransactionId'].map(train.groupby('TransactionId').agg('count')['Number_Of_Split_Payments'])\n",
    "test['Number_Of_Split_Payments']=test['TransactionId'].map(test.groupby('TransactionId').agg('count')['Number_Of_Split_Payments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[(train.TransactionId=='TransactionId_703')|((train.TransactionId=='TransactionId_927'))].index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets drop the duplicate rows with the same transaction ID and keep the last one. (as in with the latest payment installment )\n",
    "train.drop_duplicates(subset=['TransactionId'],keep='last',inplace=True)\n",
    "test.drop_duplicates(subset=['TransactionId'],keep='last',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['CountryCode','Currency','CurrencyCode','SubscriptionId','ProviderId','ChannelId'],axis=1,inplace=True)\n",
    "test.drop(['CountryCode','CurrencyCode','SubscriptionId','ProviderId','ChannelId'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Count_Rejected_Loans'] = train['CustomerId'].map(train[train.TransactionStatus==0].groupby('CustomerId').LoanId.size())\n",
    "test['Count_Rejected_Loans'] = test['CustomerId'].map(train[train.TransactionStatus==0].groupby('CustomerId').LoanId.size())\n",
    "## then we should impute the columns of customers that were not found in the rejected list with 0 as in they have never been rejected.\n",
    "train.Count_Rejected_Loans.fillna(value=0,inplace=True)\n",
    "test.Count_Rejected_Loans.fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amine\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## group train/test together to perform cumulative count\n",
    "all_data=pd.concat((train,test))\n",
    "## Initialize and compute values for the new feature\n",
    "all_data['Cumulative_Reject']=0\n",
    "all_data.loc[all_data.TransactionStatus==0,'Cumulative_Reject'] = all_data[all_data.TransactionStatus==0].groupby('CustomerId').cumcount()\n",
    "## Separate all_data into train and test\n",
    "train1=all_data[:len(train)]\n",
    "test1=all_data[len(train):]\n",
    "train['Cumulative_Reject']=0\n",
    "test['Cumulative_Reject']=0\n",
    "train['Cumulative_Reject']=train1['Cumulative_Reject']\n",
    "test['Cumulative_Reject']=test1['Cumulative_Reject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchasestats=train[train.TransactionStatus==0].groupby('CustomerId').Value.agg(('mean','std','min','max'))\n",
    "train['prchs_mean']=train['CustomerId'].map(purchasestats['mean'])\n",
    "train['prchs_std']=train['CustomerId'].map(purchasestats['std'])\n",
    "train['prchs_max']=train['CustomerId'].map(purchasestats['max'])\n",
    "train['prchs_min']=train['CustomerId'].map(purchasestats['min'])\n",
    "test['prchs_mean']=test['CustomerId'].map(purchasestats['mean'])\n",
    "test['prchs_std']=test['CustomerId'].map(purchasestats['std'])\n",
    "test['prchs_max']=test['CustomerId'].map(purchasestats['max'])\n",
    "test['prchs_min']=test['CustomerId'].map(purchasestats['min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuegroups=mask.groupby('CustomerId').Value.agg(('mean','std','min','max'))\n",
    "train['mean_cus_transac']=train['CustomerId'].map(valuegroups['mean'])\n",
    "train['std_cus_transac']=train['CustomerId'].map(valuegroups['std'])\n",
    "train['min_cus_transac']=train['CustomerId'].map(valuegroups['min'])\n",
    "train['max_cus_transac']=train['CustomerId'].map(valuegroups['max'])\n",
    "test['mean_cus_transac']=test['CustomerId'].map(valuegroups['mean'])\n",
    "test['std_cus_transac']=test['CustomerId'].map(valuegroups['std'])\n",
    "test['min_cus_transac']=test['CustomerId'].map(valuegroups['min'])\n",
    "test['max_cus_transac']=test['CustomerId'].map(valuegroups['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Day_Of_Week']= train.TransactionStartTime.dt.weekday\n",
    "test['Day_Of_Week'] =test.TransactionStartTime.dt.weekday\n",
    "train['Day_in_month']=train.TransactionStartTime.dt.day\n",
    "test['Day_in_month']=test.TransactionStartTime.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "datemin = date(2018,9,21)\n",
    "datemax= date(2019,7,17)\n",
    "(datemax-datemin).days\n",
    "datesinc=pd.DataFrame(columns=['date','inc_value'])\n",
    "datesinc.loc[0,'inc_value']=1\n",
    "datesinc.loc[0,'date']=datemin\n",
    "from datetime import timedelta\n",
    "for i in range(2,301):\n",
    "    datesinc.loc[i-1,'inc_value']=i\n",
    "    datesinc.loc[i-1,'date']=datemin + timedelta(days=i-1)\n",
    "train['inc_value_date']=train.TransactionStartTime.dt.date.map(datesinc.set_index('date').inc_value)\n",
    "test['inc_value_date']=test.TransactionStartTime.dt.date.map(datesinc.set_index('date').inc_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.inc_value_date = train.inc_value_date.astype(np.int64)\n",
    "test.inc_value_date = test.inc_value_date.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=train[(train.TransactionStatus==1)&(train.TransactionStartTime<train.DueDate)].groupby('CustomerId').agg(('count','mean','std','min','max')).Value\n",
    "#train['number_transac_before_due']=train['CustomerId'].map(aa['count'])\n",
    "train['before_due_mean'] = train['CustomerId'].map(aa['mean'])\n",
    "train['before_due_std'] = train['CustomerId'].map(aa['std'])\n",
    "train['before_due_min'] = train['CustomerId'].map(aa['min'])\n",
    "train['before_due_max'] = train['CustomerId'].map(aa['max'])\n",
    "test['before_due_mean'] = test['CustomerId'].map(aa['mean'])\n",
    "test['before_due_std'] = test['CustomerId'].map(aa['std'])\n",
    "test['before_due_min'] = test['CustomerId'].map(aa['min'])\n",
    "test['before_due_max'] = test['CustomerId'].map(aa['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cnt_missed_payment']=0\n",
    "train.loc[train.DueDate<train.PaidOnDate,'Cnt_missed_payment']=train[train.DueDate<train.PaidOnDate].groupby('CustomerId').cumcount()\n",
    "test['Cnt_missed_payment']=test['CustomerId'].map(train.groupby('CustomerId').agg('max').Cnt_missed_payment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train[train.IsDefaulted.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CustomerId', 'TransactionStartTime', 'Value', 'Amount',\n",
       "       'TransactionId', 'BatchId', 'ProductId', 'ProductCategory',\n",
       "       'TransactionStatus', 'IssuedDateLoan', 'LoanId', 'InvestorId',\n",
       "       'LoanApplicationId', 'ThirdPartyId', 'Number_Of_Split_Payments',\n",
       "       'Count_Rejected_Loans', 'Cumulative_Reject', 'prchs_mean', 'prchs_std',\n",
       "       'prchs_max', 'prchs_min', 'mean_cus_transac', 'std_cus_transac',\n",
       "       'min_cus_transac', 'max_cus_transac', 'Day_Of_Week', 'Day_in_month',\n",
       "       'inc_value_date', 'before_due_mean', 'before_due_std', 'before_due_min',\n",
       "       'before_due_max', 'Cnt_missed_payment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['CustomerId', #'TransactionStartTime', \n",
    "            'Value', #'Amount',\n",
    "       #'TransactionId', #'BatchId', \n",
    "             'ProductId',\n",
    "       'ProductCategory', #'TransactionStatus', \n",
    "            #'IssuedDateLoan',\n",
    "       #'LoanId', 'InvestorId', 'LoanApplicationId', 'ThirdPartyId',\n",
    "       'Number_Of_Split_Payments', 'Count_Rejected_Loans', \n",
    "           #'Cumulative_Reject',\n",
    "       #'prchs_mean', 'prchs_std', 'prchs_max', 'prchs_min', \n",
    "            'mean_cus_transac','std_cus_transac', 'min_cus_transac', 'max_cus_transac', \n",
    "            'Day_Of_Week','Day_in_month', 'inc_value_date', 'before_due_mean', 'before_due_std',\n",
    "       'before_due_min', 'before_due_max', \n",
    "            'Cnt_missed_payment'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "oce = ce.OneHotEncoder(cols=['ProductId','ProductCategory'])\n",
    "tce = ce.TargetEncoder(cols=['CustomerId'],smoothing=40,min_samples_leaf=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features]\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train.IsDefaulted.copy()\n",
    "X = oce.fit_transform(X)\n",
    "X = tce.fit_transform(X,y)\n",
    "X_test = oce.transform(X_test)\n",
    "X_test = tce.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=6,learning_rate=0.05,n_estimators=200)\n",
    "xgb.fit(X,y)\n",
    "preds=xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.DataFrame(columns=['TransactionId','IsDefaulted'])\n",
    "sample_submission['TransactionId'] = test['TransactionId']\n",
    "sample_submission['IsDefaulted'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('C:/Users/Amine/Desktop/STB_submit_baseline8.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_submission[sample_submission.IsDefaulted>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
